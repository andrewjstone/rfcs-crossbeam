# Summary

This is a proposal for an improved epoch GC. It aims to solve weaknesses in the
current implementation and add new features that are necessary for building
more complex data structures.

# Motivation

Data structures pin the current thread before using `Atomic`s, and unpin it
when they're finished. Pinning effectively stalls garbage collection - it
prevents destruction of any new garbage until the thread gets unpinned.

The two core operations of the epoch GC are:

1. Pinning. We pin the current thread, execute a block of code that uses
`Atomic`s, and finally unpin the thread. Any garbage created while the
thread is pinned will not be destroyed until it gets unpinned.

2. Adding garbage. A heap-allocated object is added to the GC. As soon as
all pinned threads get unpinned, it will become unreachable and therefore
safe to be destroyed.

Garbage collection is occasionally automatically triggered by both of
these operations.

### Slow thread pinning

To pin and unpin a thread, the current implementation executes 6 atomic
operations and a full fence. [Coco](https://github.com/stjepang/coco) proves
that pinning can be more efficient: it executes 3 atomic operations and
a full fence.

Benchmark (on a modern Intel x86-64):

```
test pin_coco      ... bench:          11 ns/iter (+/- 0)
test pin_crossbeam ... bench:          27 ns/iter (+/- 1)
```

### Long GC pauses

Crossbeam collects garbage just after pinning the current thread, if it notices that
the global and thread-local epoch differ. Then it destroys all garbage
stored in one of the three thread-local garbage vectors.

A lot of garbage (hundreds of thousands of items) might accumulate in
thread-local vectors. Pauses due to garbage destruction might take tens or
hundreds of milliseconds. Long pauses are undesirable, so it'd be better to
collect garbage incrementally.

But when and how much garbage should be collected, exactly?
A good time to collect some garbage is just after producing garbage.
By collecting more garbage than was produced we can make sure that the
speed of garbage collection is higher than the speed of garbage production.

Another related problem are unpredictable pauses. Sometimes it is desirable
to avoid garbage collection altogether for a period of time, e.g. when
executing database transactions or sending orders in HFT scenarios.
Even though a new GC might try very hard to reduce the length of pauses,
it may still be beneficial to forbid garbage collection in some critical
sections.

### Garbage stuck thread-locally

Another problem is that garbage that gets stored in thread-local vectors
will never get collected unless the thread exits or gets pinned.

A better idea would be to allow keeping only a bounded number of items in
thread-local vectors, and spilling the rest into globally shared vectors.

Very large objects (e.g. arrays backing Chase-Lev deques or hash tables)
should probably avoid thread-local vectors altogether and go straight into
a global vector, where any thread can collect them. Large objects must be
collected more eagerly than small objects in order to reclaim wasted memory
faster.

### Tracking epochs

Crossbeam's GC must work seamlessly and be as unintrusive as possible.
For example, using process signals to coordinate garbage collection is out
of the question. Blocking algorithms are also unacceptable.

Epochs are tracked by a combination of global state and
thread-local storage. There are multiple approaches to tracking epochs
among threads and collecting garbage:

#### Approach 1

This is what Crossbeam's current implementation does:

Epochs are integers in the interval [0, 2], and they wrap around so that
0 comes after 2. After a thread announces that it just got pinned (using
a thread-local `AtomicBool`), it sets its thread-local epoch to the
global one.

Threads store garbage in one of their three thread-local vectors: garbage
goes into the vector associated with the current thread-local epoch.
If a thread notices that the global and its thread-local epoch differ,
it may empty all garbage stored in its vector associated with the global
epoch.

This means that one vector is always getting filled and the other two are
blocked. Only when the global epoch changes one of the vectors can be
emptied and becomes the new filling vector.

#### Approach 2

This approach is similar to the first one, but the difference is in how
we keep track of which thread is in which epoch. There are three global
counters that count how many threads are in which epoch. However, all
three counters and the current global epochs are packed into a single
`AtomicUsize`.

To pin a thread, we perform a CAS to increment the counter for the current
epoch. To unpin a thread, we perform a CAS to decrement that same counter.
Occasionally, if all counters except the one for the global epoch are zero,
we can perform a CAS to increment the global epoch.

It's possible to optimize this with various tricks and sometimes use
`fetch_add` instead of CAS, but the main drawback of this approach is that
there's too much contention on the single global atomic integer.

This approach can use very few atomic instructions and is indeed fast if
only one thread is working, but becomes slow in concurrent scenarios.

#### Approach 3

There is no global epoch. Each thread has its own thread-local epoch.
Whenever a thread gets pinned or unpinned, it just increments its epoch.
Whenever an object becomes garbage, it is tagged with a vector of all
current thread-local epochs. The object may be destroyed as soon as
the epoch of every pinned thread changes.

This approach is used in [ssmem](https://github.com/LPD-EPFL/ssmem), an
epoch-based memory allocator written in C.

A potential problem is that these vectors might consume a lot of memory
if we have a lot of threads running. It's also difficult to efficiently
check whether garbage is safe for destruction. Pinning is very fast,
though.

#### Approach 4

There is a globally shared `AtomicUsize` representing the global epoch.
When a thread gets pinned, it loads the global epoch and stores it into
its thread-local `AtomicUsize`, indicating what was the global epoch
when it got pinned. To unpin a thread, we just set the thread-local
atomic to a sentinel value.

Occasionally, we check whether all currently pinned threads have seen the
latest global epoch - if so, it is incremented. When an object becomes
garbage, it is tagged with the current global epoch. As soon as the
global epoch advances twice (differs from the tag by two), it will be
safe for destruction.

Pinning is fast, and the memory overhead of garbage tagging is minimal.
This approach is used in SQL Server
([pdf](http://cidrdb.org/cidr2015/Papers/CIDR15_Paper15.pdf), section 5.2)
and in [Coco](https://github.com/stjepang/coco) with great success, and
overall seems like the clear winner.

### Garbage objects

Pretty much all garbage objects can be seen through one of the three categories:

1. Small objects. These are e.g. nodes in a queue or a skiplist. Such objects
are generally quick to destroy and consume small amounts of memory.

2. Medium objects. These are nodes in a B-tree, radix tree, or perhaps
hashtable buckets. They consist of dozens of smaller objects.

3. Large objects. These are usually so large that they hold the whole data
structure - e.g. arrays backing Chase-Lev deques. If they run destructors,
they may be slow to destroy. Moreover, since they consume a lot of
memory, we really want them destroyed as soon as possible.

### Thread-local and global garbage

When a thread unlinks an object from a data structure, it becomes garbage.
It will be safe to be destroyed later, but which thread should destroy it?

Implementations of `free(ptr)` in memory allocators usually simply add the
pointer to a thread-local cache. Since the current thread was the last one
using the object, it's in the CPU cache of the core on which it is running,
so it'd be best for this thread to be the first to reuse the object in the future.
Only if thread-local garbage cache becomes full (too many or too large objects),
it flushes objects into the main (global) pool of objects, where any thread
can reuse them.

Crossbeam should try doing something similar, or at least playing along with
the allocator. If it creates garbage, it should be first added to a thread-local
cache (Crossbeam's cache, not allocator's cache). If the cache becomes full,
we flush some of the oldest garbage into the global pool of garbage, where
any thread can pick it up and destroy it.

To collect some garbage, a thread should pick up some garbage from its
thread-local cache and, if it has time for more, also steal some from the
global pool.

### Destructors

Concurrent data structures can be placed into two categories:

1. Those that disallow borrows. They hold elements temporarily until someone
comes and removes them. The ownership is then fully passed over. Examples are
queues, deques, and stacks. Threads only add and remove elements; they don't
borrow them nor peek into them.

2. Those that allow borrows. They keep ownership of their elements forever.
Threads can add new elements, remove them, and look at them (borrow). However,
they can't "steal" them. Typical examples are sets and maps.

Whether a data structure falls into the first or the second category has deep
consequences on deferred destructors.

When an element is popped from a Michael-Scott queue, all we need to do is
deallocate the removed node that held it (at a later safe time). We don't
destruct elements. Only when the queue itself is getting destructed we remove all
remaining elements and destruct them one by one.

However, if a thread removes an element from a map, the map doesn't pass
ownership over: the element is simply borrowed because there might be other
threads holding references to it as well. Therefore, the map must add the
removed element to the epoch GC and tell it to not only deallocate, but run
the destructor for it as well. Crossbeam's GC doesn't support destructors at
the moment - this has yet to be implemented.

The epoch GC will destroy garbage at some time in the future, but it doesn't
guarantee when exactly. Things get tricky when the GC destructs objects of type
`T` that have non-static references. We must either restrict the type (`T: 'static`),
or we somehow make sure that `T` doesn't touch its non-static references when it gets
destructed, or `T` gets destructed no later than its parent data structure.

There are three ways of solving this problem:

1. Restrict the type with `T: 'static`. This is too restrictive for e.g. keys
in maps: it makes sense to have keys of type `&'a str`. But such references
don't have destructors anyway, so we can simply restrict the type with `T: StaticOrCopy`,
where `StaticOrCopy` is a trait implemented for all `T` such that `T: 'static`
or `T: Copy`. That bound is not as restrictive anymore, and works well with
deferred destructors in the epoch GC.

2. Run destructors for all `T`, even if `T` holds non-static references.
In order to ensure that destructors don't get run too late, the destructor
of the parent data structure waits until the global epoch is advanced and all
garbage is destroyed. But this solution is slow and makes the algorithm
blocking.

3. Introduce some kind of storage for garbage produced by each data structure.
In other words, this storage is like a specialized GC tied to the data structure.
Produced garbage goes into this storage, and all remaining garbage is destroyed
at the as soon as the data structure is dropped. This solves the problem of
lifetimes completely, but the main drawback is that the specialized GCs might be
heavyweight (e.g. having 10,000 concurrent maps will consume tons of memory if
every GC has per-thread caches) and possibly less performant than the global GC.

Another argument for specialized GCs (local to each data structure) is that
destructors of `T` can be unpredictable. They might do all sorts of things,
e.g. write to files, block, or simply take a long time. It might be unfair
if some other threads that perhaps that don't even touch the data structure
pick up some garbage produced by it and possibly stutter due to slow destructors.

However, such scenarios may be unlikely because keys in maps usually don't have
destructors at all, or their destructors are very simple anyway. On the other hand,
values stored in maps might have more intensive destructors. But it's
debatable whether the epoch GC should destroy values in maps at all - 
with a reference counting mechanism values may be destroyed in a predictable manner
without the help of epoch GC.

Restricting types (the first option) is a very plausible solution as well.
Perhaps it'd be a good idea to have something like this provided by Crossbeam:

```rust
pub unsafe trait DeferredDropSafe {}
unsafe impl<T: Send + Copy> DeferredDropSafe for T {}
unsafe impl<T: Send + 'static> DeferredDropSafe for T {}
```

Now, a concurrent map might have the following interface: <br>
```rust
SkiplistMap<K: Ord + DeferredDropSafe, V>
```

If we don't use use a reference counting mechanism for values, the interface is: <br>
```rust
SkiplistMap<K: Ord + DeferredDropSafe, V: DeferredDropSafe>
```

And with specialized GCs it becomes fully generic: <br>
```rust
SkiplistMap<K: Ord, V>
```

Finally, as an interesting side note, there is an intrinsic `std::intrinsics::needs_drop`,
which is currently unstable, but might be
[stabilized relatively soon](https://github.com/rust-lang/rust/issues/41890).
This intrinsic might help in avoiding unnecessary performance penalties in some
cases.

### Pinning

Data structures pin the current thread before using `Atomic`s, and unpin it
when they're finished. Pinning effectively stalls garbage collection - it
prevents destruction of new garbage until the thread gets unpinned.

For that reason pinning must be very quick, and a general guideline in ensuring
that is that user code shouldn't run inside pinned scopes, unless absolutely necessary.
For example, consider the following method:

```rust
impl<K, V> SkiplistMap<K, V> {
    fn get_or_insert_with<F: FnMut() -> V>(
        &self,
        k: K,
        f: F,
    ) -> Cursor<K, V>;
}
```

This method returns the element with key `k`, initializing a new one if it
doesn't exist. We don't know whether initialization will take a long time
or even block execution, so it's ba good idea to execute it outside a pinned block
of code. That said, sometimes it's difficult to isolate user code execution outside
pinned blocks, and this isolation might come with a performance penalty as well.

Another problem is garbage destruction. To collect garbage, we steal some
of it from concurrent data structures holding it. This, of course, happens
within a pinned block. It makes sense to store stolen garbage into a
thread-local staging area and destroy it just after the moment the thread
gets unpinned.

Pinning is also reentrant - we can have nested `pin` blocks:

```rust
epoch::pin(|scope| {
    epoch::collect(scope);

    // This inner call doesn't pin nor unpin the thread - it becomes a noop.
    epoch::pin(|scope| {
        epoch::collect(scope);
    });
});
// All garbage we have collected within the outer block gets destroyed now.
```

### Delayed garbage collection

Sometimes we really care about having code with low and predictable latency.
There might be times when it is unacceptable to spend any time on garbage collection
(at the expense of times when it becomes safe to do so).
For example, .NET Framework has methods `GC.TryStartNoGCRegion` and `GC.EndNoGCRegion`
for pausing the GC in very critical sections of code.

Pauses due to incremental garbage collection in Crossbeam may be very small,
but there could be scenarios where they become unacceptable nonetheless.
A function that pauses garbage destruction might be used like this:

```rust
epoch::delay_gc(|| {
    // Latency-critical section.
    epoch::pin(|scope| {
        // Any collected garbage simply gets stashed away into the thread-local staging area.
    });
});
// All garbage collected within the `delay_gc` scope gets destroyed.
```

Note that `epoch::pin` itself implies `epoch::delay_gc`.

To prevent garbage destruction, we could've just as well enclosed
the latency-critical section with `epoch::pin`, but that would prevent collection
of newly produced garbage.

What we really want is to simply keep garbage collected by the current thread
until the end of the scope, and then destroy all of it at once.
That is the purpose of the suggested `epoch::delay_gc` function.

### Oversubscription

If we have more threads than there are available processors, the epoch
GC is accumulating a lot of garbage. This is understandable, as there are
more threads producing garbage, and some threads are even preempted while
they are pinned, thus slowing the GC down.

Overall, simple tests indicate that the maximum amount of accumulated garbage
probably scales linearly with the number of running threads. That is
not *too* bad, but one should keep in mind that it's best to have about
the same number of threads using the epoch GC as there are processors.

Other memory management schemes like hazard pointers don't suffer as much
because they accumulate less (and a bounded amount of) garbage, but
their memory consumption scales linearly with the number of threads as well.

# Detailed design

TODO

### Thread registration

Threads are registered the first time they get pinned. There is a global
lock-free singly-linked list of thread entries. On registration a thread
adds its entry to the list, which contains its thread-local data.

When a thread exits, its thread-local storage gets destructed, and some
drop code is triggered. The thread removes its entry from the list, and
moves its thread-local garbage into the global garbage.

### The interface

TODO

# Drawbacks

TODO Why should we not do this?

# Alternatives

TODO dedicated GC threads?

# Unresolved questions

### Extreme use cases

TODO low latency, high thread counts, tuning knobs

### `sys_membarrier`

TODO

### Different crate versions

TODO linking different versions of crossbeam-epoch
